{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f2f9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./COGS/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./COGS/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_tiny_df = pd.read_csv(\"./COGS/dev_tiny.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./COGS/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./COGS/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_lexical_df = pd.read_csv(\"./COGS/gen_lexical.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_structural_df = pd.read_csv(\"./COGS/gen_structural.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "train_df_original = pd.read_csv(\"./COGS/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "774b0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.second_looks_utils import *\n",
    "from utils.train_utils import *\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "existing_digit_pool = set([])\n",
    "# loading target vocab to random sample our variable names\n",
    "for k, v in load_vocab(\"./data/tgt_vocab.txt\").items():\n",
    "    if k.isnumeric():\n",
    "        existing_digit_pool.add(k)\n",
    "existing_digit_pool = list(existing_digit_pool)\n",
    "\n",
    "def translate(text, phi):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, f\"LAMBDA a . {phi} ( a )\"\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        if len(phi.split()) == 7:\n",
    "            return text, phi\n",
    "        phi_split = phi.split(text)\n",
    "        cleaned_phi = []\n",
    "        for chunk in phi_split:\n",
    "            if \"LAMBDA\" in chunk:\n",
    "                cleaned_phi += [chunk.strip()]\n",
    "            else:\n",
    "                verb_args = chunk.strip(\" .\").split()[2]\n",
    "                cleaned_phi += [chunk.strip(\" .\")]\n",
    "        return text, \" \".join(cleaned_phi[:1] + [f\"{text} ( {verb_args} ) AND\"] + cleaned_phi[1:])\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "            if \"x _\" not in d['entvar']:\n",
    "                d['entvar_name'] = d['entvar']\n",
    "                assert text_split.count(d['entvar']) == 1\n",
    "                name_idx = text_split.index(d['entvar'])\n",
    "                d['entvar'] = f\"x _ {name_idx}\"\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                def_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            if f\"{d['pred']} ( {d['eventvar']} )\" not in role_terms:\n",
    "                role_terms += [f\"{d['pred']} ( {d['eventvar']} )\"]\n",
    "            role_terms += [f\"{d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "            if \"entvar_name\" in d:\n",
    "                def_terms += [f\"{d['entvar_name']} ( {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            role_terms += [f\"nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))\n",
    "\n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    # final step, remove biases\n",
    "    current_digit_pool = set([])\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            current_digit_pool.add(t)\n",
    "    current_digit_pool = list(current_digit_pool)\n",
    "    random.shuffle(current_digit_pool)\n",
    "    sample_random_digit = random.sample(existing_digit_pool, k=len(current_digit_pool))\n",
    "    digit_mapping = dict(zip(current_digit_pool, sample_random_digit))\n",
    "\n",
    "    new_terms = []\n",
    "    for t in terms.split():\n",
    "        if t == \"_\" or t == \"x\":\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            new_terms += [digit_mapping[t]]\n",
    "        else:\n",
    "            new_terms += [t]\n",
    "\n",
    "    terms = \" \".join(new_terms)\n",
    "    return text, terms\n",
    "\n",
    "sampled_n = 5\n",
    "append_k = 3072\n",
    "\n",
    "train_dfs = []\n",
    "for i in range(sampled_n):\n",
    "    train_df_i = train_df.copy()\n",
    "    train_df_i[['sentence', 'LF']] = train_df_i[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "    train_dfs += [train_df_i]\n",
    "\n",
    "train_df_original[['sentence', 'LF']] = train_df_original[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF']] = dev_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF']] = test_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF']] = gen_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_lexical_df[['sentence', 'LF']] = gen_lexical_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_structural_df[['sentence', 'LF']] = gen_structural_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "dev_tiny_df[['sentence', 'LF']] = dev_tiny_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "\n",
    "def reindex(LFs, existing_digit_pool):\n",
    "    curr_digit = set([])\n",
    "    for i in range(len(LFs)):\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                curr_digit.add((i, int(item)))\n",
    "    sampled_digits = random.sample(existing_digit_pool, k=len(curr_digit))\n",
    "    digit_map = {}\n",
    "    idx = 0\n",
    "    for d in list(curr_digit):\n",
    "        digit_map[d] = sampled_digits[idx]\n",
    "        idx += 1\n",
    "    \n",
    "    reindex_LFs = []\n",
    "    for i in range(len(LFs)):\n",
    "        new_LFs = []\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                new_LFs += [digit_map[(i, int(item))]]\n",
    "            else:\n",
    "                new_LFs += [item]\n",
    "        reindex_LFs += [\" \".join(new_LFs)]\n",
    "        \n",
    "    new_LF_prefix = []\n",
    "    new_LF_body_role = []\n",
    "        \n",
    "    for i in range(len(reindex_LFs)):\n",
    "        new_LF_prefix.extend(reindex_LFs[i].split(\" ; \")[:-1])\n",
    "        for term in reindex_LFs[i].split(\" ; \")[-1].split(\" AND \"):\n",
    "            new_LF_body_role += [term]\n",
    "                \n",
    "    new_LF_body = new_LF_body_role\n",
    "        \n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" AND \".join(new_LF_body)\n",
    "\n",
    "start_indexes = [i*6 for i in range(append_k)]\n",
    "append_data = []\n",
    "\n",
    "for i in range(sampled_n):\n",
    "    train_df_sorted = train_dfs[i].sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        conj_1 = train_df_sorted.iloc[-2-start_index].sentence\n",
    "        if conj_1.split()[0] in {'The', 'A'}:\n",
    "            conj_1_first = conj_1[0].lower()\n",
    "        else:\n",
    "            conj_1_first = conj_1[0]\n",
    "            \n",
    "        conj_2 = train_df_sorted.iloc[-3-start_index].sentence\n",
    "        if conj_2.split()[0] in {'The', 'A'}:\n",
    "            conj_2_first = conj_2[0].lower()\n",
    "        else:\n",
    "            conj_2_first = conj_2[0]\n",
    "            \n",
    "        append_data += [\n",
    "            [train_df_sorted.iloc[-1-start_index].sentence[:-1]+\\\n",
    "            conj_1_first+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[1:-1]+\\\n",
    "            conj_2_first+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    train_df_sorted.iloc[-1-start_index].LF,\n",
    "                    train_df_sorted.iloc[-2-start_index].LF,\n",
    "                    train_df_sorted.iloc[-3-start_index].LF\n",
    "                ], existing_digit_pool\n",
    "            ),\n",
    "            'length_ood']\n",
    "        ]\n",
    "append_data = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_dfs)\n",
    "train_df = pd.concat([train_df, append_data])\n",
    "train_df = train_df.drop_duplicates()\n",
    "\n",
    "dataset_postfix = \"COGS\"\n",
    "train_df_original.to_csv(f'./{dataset_postfix}/RECOGStrain.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/RECOGSdev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/RECOGStest.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/RECOGSgen.tsv', sep='\\t', index=False, header=False)\n",
    "gen_lexical_df.to_csv(f'./{dataset_postfix}/RECOGSgen_lexical.tsv', sep='\\t', index=False, header=False)\n",
    "gen_structural_df.to_csv(f'./{dataset_postfix}/RECOGSgen_structural.tsv', sep='\\t', index=False, header=False)\n",
    "dev_tiny_df.to_csv(f'./{dataset_postfix}/RECOGSdev_tiny.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab177ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
